{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a9af167",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e45c2e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233e228e",
   "metadata": {},
   "source": [
    "# Logistic Regression Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e64e40ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, x):\n",
    "        self.theta = np.random.randn(x.shape[1] + 1)  # Generate random weights\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    def gradientDescent(self, x, y, alpha=0.01):\n",
    "        h = self.sigmoid(np.dot(x, self.theta.T))     # hypothesis\n",
    "        error = h - y\n",
    "        self.theta -= alpha * (1 / len(y)) * np.dot(x.T, error)\n",
    "    def cost(self, x, y):  \n",
    "        h = self.sigmoid(np.dot(x, self.theta.T))  # hypothesis\n",
    "        return (-1) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "    def predictClass(self, x):\n",
    "        x = np.column_stack((x, np.ones((x.shape[0]))))     # Add bias term during prediction\n",
    "        h = self.sigmoid(np.dot(x, self.theta.T))     # hypothesis\n",
    "        return np.where(h >= 0.5, 1, 0)\n",
    "    def predictConfidence(self, x):\n",
    "        return self.sigmoid(np.dot(x, self.theta.T))\n",
    "    def getWeights(self):\n",
    "        return self.theta\n",
    "    def train(self, x, y, alpha=0.01, numIters=1000, printInterval=20):\n",
    "        x = np.column_stack((x, np.ones((x.shape[0]))))  #Add bias term for training by adding extra column at the end of x\n",
    "        for i in range(numIters):\n",
    "            self.gradientDescent(x, y)\n",
    "            if printInterval is not None and i % printInterval == 0:\n",
    "                loss = self.cost(x, y)  \n",
    "                print(f\"Iteration: {i}, Loss: {loss:.4f}\")\n",
    "        probabilities = self.predictConfidence(x)             #probabilities and weights after training\n",
    "        print(\"\\nProbabilities of x examples belonging to the class:\\n\",probabilities)\n",
    "        print(\"\\nFinal weights:\\n\",self.theta, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60052fde",
   "metadata": {},
   "source": [
    "# Loading Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c2795d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4929593e",
   "metadata": {},
   "source": [
    "# Training the Model using One-vs-All approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d775994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: 127.9264\n",
      "Iteration: 20, Loss: 106.9436\n",
      "Iteration: 40, Loss: 93.1329\n",
      "Iteration: 60, Loss: 81.1521\n",
      "Iteration: 80, Loss: 70.9063\n",
      "Iteration: 100, Loss: 62.2676\n",
      "Iteration: 120, Loss: 55.0426\n",
      "Iteration: 140, Loss: 49.0155\n",
      "Iteration: 160, Loss: 43.9795\n",
      "Iteration: 180, Loss: 39.7524\n",
      "Iteration: 200, Loss: 36.1820\n",
      "Iteration: 220, Loss: 33.1445\n",
      "Iteration: 240, Loss: 30.5407\n",
      "Iteration: 260, Loss: 28.2917\n",
      "Iteration: 280, Loss: 26.3350\n",
      "Iteration: 300, Loss: 24.6207\n",
      "Iteration: 320, Loss: 23.1088\n",
      "Iteration: 340, Loss: 21.7672\n",
      "Iteration: 360, Loss: 20.5698\n",
      "Iteration: 380, Loss: 19.4956\n",
      "Iteration: 400, Loss: 18.5271\n",
      "Iteration: 420, Loss: 17.6498\n",
      "Iteration: 440, Loss: 16.8518\n",
      "Iteration: 460, Loss: 16.1231\n",
      "Iteration: 480, Loss: 15.4553\n",
      "Iteration: 500, Loss: 14.8410\n",
      "Iteration: 520, Loss: 14.2743\n",
      "Iteration: 540, Loss: 13.7500\n",
      "Iteration: 560, Loss: 13.2634\n",
      "Iteration: 580, Loss: 12.8107\n",
      "Iteration: 600, Loss: 12.3886\n",
      "Iteration: 620, Loss: 11.9940\n",
      "Iteration: 640, Loss: 11.6244\n",
      "Iteration: 660, Loss: 11.2774\n",
      "Iteration: 680, Loss: 10.9512\n",
      "Iteration: 700, Loss: 10.6438\n",
      "Iteration: 720, Loss: 10.3537\n",
      "Iteration: 740, Loss: 10.0795\n",
      "Iteration: 760, Loss: 9.8199\n",
      "Iteration: 780, Loss: 9.5738\n",
      "Iteration: 800, Loss: 9.3401\n",
      "Iteration: 820, Loss: 9.1180\n",
      "Iteration: 840, Loss: 8.9065\n",
      "Iteration: 860, Loss: 8.7051\n",
      "Iteration: 880, Loss: 8.5128\n",
      "Iteration: 900, Loss: 8.3292\n",
      "Iteration: 920, Loss: 8.1537\n",
      "Iteration: 940, Loss: 7.9856\n",
      "Iteration: 960, Loss: 7.8247\n",
      "Iteration: 980, Loss: 7.6704\n",
      "\n",
      "Probabilities of x examples belonging to the class:\n",
      " [0.95519191 0.96086442 0.06976027 0.87486941 0.89669237 0.01394558\n",
      " 0.06061731 0.90797317 0.92400436 0.94513923 0.01271484 0.06541104\n",
      " 0.04352499 0.95272537 0.92529371 0.08981791 0.01621538 0.0110595\n",
      " 0.06273281 0.00732209 0.06526015 0.00215463 0.04296038 0.91904974\n",
      " 0.00157816 0.06744814 0.91380222 0.9281893  0.86608068 0.0781784\n",
      " 0.01978831 0.81089695 0.83553903 0.84894807 0.05382377 0.89802118\n",
      " 0.07175142 0.00383381 0.88526037 0.06227386 0.01207184 0.95921076\n",
      " 0.028357   0.01271484 0.0534517  0.04114107 0.01543495 0.0507929\n",
      " 0.9130557  0.13376359 0.01053366 0.90244384 0.92125117 0.07659826\n",
      " 0.02433376 0.86493466 0.01319756 0.89593417 0.88476136 0.03310332\n",
      " 0.23414311 0.00929834 0.01372119 0.00584117 0.00485944 0.060334\n",
      " 0.84873107 0.90827568 0.01038481 0.00645566 0.85682174 0.91825207\n",
      " 0.87854891 0.01707533 0.01132262 0.9015693  0.01070644 0.00536124\n",
      " 0.90280239 0.03238938 0.03748905 0.02481483 0.05115579 0.01583329\n",
      " 0.91505312 0.01407079 0.07263214 0.0163932  0.14647299 0.07735347\n",
      " 0.04137107 0.79459116 0.04267855 0.03572045 0.92371742 0.08088409\n",
      " 0.00309161 0.0099392  0.91889621 0.14190606 0.00952773 0.00760509\n",
      " 0.9231637  0.0094468  0.87310883 0.03322508 0.00395322 0.00768228\n",
      " 0.06978144 0.01033449 0.03232636 0.06496438 0.01814585 0.01719879\n",
      " 0.87183397 0.08812437 0.02323434 0.96672535 0.07069394 0.0059986 ]\n",
      "\n",
      "Final weights:\n",
      " [ 0.22585286  0.91120864 -1.62955948 -0.1067381   0.39115637] \n",
      "\n",
      "\n",
      "Accuracy for class 1: 1.0000\n",
      "\n",
      "Iteration: 0, Loss: 437.6832\n",
      "Iteration: 20, Loss: 260.8566\n",
      "Iteration: 40, Loss: 106.7539\n",
      "Iteration: 60, Loss: 68.3646\n",
      "Iteration: 80, Loss: 67.1830\n",
      "Iteration: 100, Loss: 66.6796\n",
      "Iteration: 120, Loss: 66.2505\n",
      "Iteration: 140, Loss: 65.8811\n",
      "Iteration: 160, Loss: 65.5621\n",
      "Iteration: 180, Loss: 65.2858\n",
      "Iteration: 200, Loss: 65.0456\n",
      "Iteration: 220, Loss: 64.8364\n",
      "Iteration: 240, Loss: 64.6535\n",
      "Iteration: 260, Loss: 64.4932\n",
      "Iteration: 280, Loss: 64.3522\n",
      "Iteration: 300, Loss: 64.2281\n",
      "Iteration: 320, Loss: 64.1183\n",
      "Iteration: 340, Loss: 64.0211\n",
      "Iteration: 360, Loss: 63.9346\n",
      "Iteration: 380, Loss: 63.8576\n",
      "Iteration: 400, Loss: 63.7889\n",
      "Iteration: 420, Loss: 63.7273\n",
      "Iteration: 440, Loss: 63.6719\n",
      "Iteration: 460, Loss: 63.6221\n",
      "Iteration: 480, Loss: 63.5771\n",
      "Iteration: 500, Loss: 63.5363\n",
      "Iteration: 520, Loss: 63.4992\n",
      "Iteration: 540, Loss: 63.4655\n",
      "Iteration: 560, Loss: 63.4346\n",
      "Iteration: 580, Loss: 63.4063\n",
      "Iteration: 600, Loss: 63.3803\n",
      "Iteration: 620, Loss: 63.3563\n",
      "Iteration: 640, Loss: 63.3341\n",
      "Iteration: 660, Loss: 63.3134\n",
      "Iteration: 680, Loss: 63.2942\n",
      "Iteration: 700, Loss: 63.2763\n",
      "Iteration: 720, Loss: 63.2595\n",
      "Iteration: 740, Loss: 63.2436\n",
      "Iteration: 760, Loss: 63.2287\n",
      "Iteration: 780, Loss: 63.2146\n",
      "Iteration: 800, Loss: 63.2011\n",
      "Iteration: 820, Loss: 63.1884\n",
      "Iteration: 840, Loss: 63.1762\n",
      "Iteration: 860, Loss: 63.1645\n",
      "Iteration: 880, Loss: 63.1533\n",
      "Iteration: 900, Loss: 63.1425\n",
      "Iteration: 920, Loss: 63.1320\n",
      "Iteration: 940, Loss: 63.1220\n",
      "Iteration: 960, Loss: 63.1122\n",
      "Iteration: 980, Loss: 63.1027\n",
      "\n",
      "Probabilities of x examples belonging to the class:\n",
      " [0.07421494 0.022906   0.43751164 0.10843458 0.14095607 0.70165313\n",
      " 0.28806762 0.13473897 0.08986109 0.02589234 0.40893734 0.13046898\n",
      " 0.4020186  0.07294107 0.09990504 0.64705033 0.4337703  0.24586478\n",
      " 0.47951822 0.15965857 0.31670219 0.50093106 0.21943197 0.12878123\n",
      " 0.64091117 0.45959839 0.18120218 0.12847318 0.04753465 0.78520032\n",
      " 0.57396338 0.09096046 0.32588085 0.20003933 0.4223215  0.15572354\n",
      " 0.24885455 0.10099248 0.16271618 0.43822654 0.55026458 0.03082178\n",
      " 0.29994109 0.40893734 0.82770737 0.18194283 0.12981557 0.69960212\n",
      " 0.05718701 0.63808182 0.55090606 0.18907072 0.26123728 0.64345516\n",
      " 0.44841084 0.25628166 0.39936874 0.14093718 0.12328909 0.17689073\n",
      " 0.58802323 0.33071265 0.42993583 0.42495775 0.63291653 0.56772018\n",
      " 0.26636517 0.21910267 0.76236946 0.31101175 0.19516759 0.13441886\n",
      " 0.22539069 0.67323995 0.1339808  0.10324607 0.44575615 0.14299617\n",
      " 0.18908268 0.35977731 0.57272922 0.48988389 0.40578132 0.26299919\n",
      " 0.05209732 0.42016821 0.326852   0.39569659 0.59811047 0.36881559\n",
      " 0.31051536 0.71847576 0.50625609 0.41985175 0.08985471 0.24834114\n",
      " 0.49646936 0.21009993 0.07936582 0.54078273 0.24472024 0.45236179\n",
      " 0.07714397 0.36855569 0.32873386 0.42729672 0.63849938 0.32516936\n",
      " 0.39711494 0.32800198 0.82087228 0.49647441 0.32968307 0.23869632\n",
      " 0.18708013 0.52898329 0.36398864 0.08363898 0.57155145 0.46838763]\n",
      "\n",
      "Final weights:\n",
      " [ 1.17130037 -2.86424171 -0.65043938  0.49337645  2.95137594] \n",
      "\n",
      "\n",
      "Accuracy for class 2: 0.7667\n",
      "\n",
      "Iteration: 0, Loss: 47.6459\n",
      "Iteration: 20, Loss: 46.6865\n",
      "Iteration: 40, Loss: 46.1960\n",
      "Iteration: 60, Loss: 45.7281\n",
      "Iteration: 80, Loss: 45.2787\n",
      "Iteration: 100, Loss: 44.8464\n",
      "Iteration: 120, Loss: 44.4299\n",
      "Iteration: 140, Loss: 44.0280\n",
      "Iteration: 160, Loss: 43.6398\n",
      "Iteration: 180, Loss: 43.2642\n",
      "Iteration: 200, Loss: 42.9005\n",
      "Iteration: 220, Loss: 42.5479\n",
      "Iteration: 240, Loss: 42.2057\n",
      "Iteration: 260, Loss: 41.8732\n",
      "Iteration: 280, Loss: 41.5500\n",
      "Iteration: 300, Loss: 41.2355\n",
      "Iteration: 320, Loss: 40.9293\n",
      "Iteration: 340, Loss: 40.6308\n",
      "Iteration: 360, Loss: 40.3397\n",
      "Iteration: 380, Loss: 40.0556\n",
      "Iteration: 400, Loss: 39.7783\n",
      "Iteration: 420, Loss: 39.5072\n",
      "Iteration: 440, Loss: 39.2423\n",
      "Iteration: 460, Loss: 38.9831\n",
      "Iteration: 480, Loss: 38.7295\n",
      "Iteration: 500, Loss: 38.4812\n",
      "Iteration: 520, Loss: 38.2380\n",
      "Iteration: 540, Loss: 37.9996\n",
      "Iteration: 560, Loss: 37.7660\n",
      "Iteration: 580, Loss: 37.5368\n",
      "Iteration: 600, Loss: 37.3120\n",
      "Iteration: 620, Loss: 37.0914\n",
      "Iteration: 640, Loss: 36.8748\n",
      "Iteration: 660, Loss: 36.6620\n",
      "Iteration: 680, Loss: 36.4531\n",
      "Iteration: 700, Loss: 36.2477\n",
      "Iteration: 720, Loss: 36.0459\n",
      "Iteration: 740, Loss: 35.8474\n",
      "Iteration: 760, Loss: 35.6523\n",
      "Iteration: 780, Loss: 35.4603\n",
      "Iteration: 800, Loss: 35.2714\n",
      "Iteration: 820, Loss: 35.0855\n",
      "Iteration: 840, Loss: 34.9025\n",
      "Iteration: 860, Loss: 34.7223\n",
      "Iteration: 880, Loss: 34.5449\n",
      "Iteration: 900, Loss: 34.3702\n",
      "Iteration: 920, Loss: 34.1980\n",
      "Iteration: 940, Loss: 34.0283\n",
      "Iteration: 960, Loss: 33.8611\n",
      "Iteration: 980, Loss: 33.6964\n",
      "\n",
      "Probabilities of x examples belonging to the class:\n",
      " [1.75851961e-03 6.53234127e-04 2.01928864e-01 5.35251066e-03\n",
      " 5.64063868e-03 6.79598711e-01 2.42590424e-01 3.05075178e-03\n",
      " 2.58136092e-03 1.32238642e-03 7.25115003e-01 2.42967798e-01\n",
      " 2.94252277e-01 1.14473682e-03 2.03201608e-03 3.09195796e-01\n",
      " 6.11045578e-01 6.59196096e-01 2.36748032e-01 6.23432788e-01\n",
      " 3.15447031e-01 8.61330881e-01 4.17218673e-01 2.77181000e-03\n",
      " 8.98906574e-01 3.17420279e-01 2.97626822e-03 2.54986993e-03\n",
      " 4.22930965e-03 4.38411298e-01 5.74943916e-01 8.68145053e-03\n",
      " 8.10603038e-03 6.14483938e-03 3.93278426e-01 3.73739755e-03\n",
      " 2.83982011e-01 6.97364898e-01 5.81147773e-03 2.80246461e-01\n",
      " 7.61580099e-01 8.13916299e-04 4.81409075e-01 7.25115003e-01\n",
      " 4.07905572e-01 4.49699271e-01 5.69433269e-01 4.64580894e-01\n",
      " 2.18896596e-03 2.71928460e-01 7.10472620e-01 3.89462363e-03\n",
      " 3.23213938e-03 3.47721431e-01 4.43902350e-01 6.42758661e-03\n",
      " 7.09895701e-01 3.99199776e-03 4.30045458e-03 4.29643611e-01\n",
      " 1.41781427e-01 6.40716395e-01 6.90017712e-01 8.40000175e-01\n",
      " 7.25832034e-01 3.97239794e-01 9.97964065e-03 5.66360433e-03\n",
      " 7.88023667e-01 6.90945633e-01 7.82765200e-03 2.80653956e-03\n",
      " 7.41331858e-03 6.31732616e-01 6.34255811e-01 4.47541541e-03\n",
      " 6.38780390e-01 7.94103116e-01 4.62992034e-03 4.46383309e-01\n",
      " 3.79367922e-01 5.17081227e-01 2.27353936e-01 5.64627360e-01\n",
      " 2.53904072e-03 5.50838468e-01 2.65095487e-01 5.66868574e-01\n",
      " 1.77130206e-01 3.44476211e-01 3.73986494e-01 1.81997645e-02\n",
      " 3.27195822e-01 5.20563892e-01 2.17069486e-03 2.67278379e-01\n",
      " 8.37257063e-01 6.33737579e-01 2.53949808e-03 2.58029599e-01\n",
      " 6.45251493e-01 6.74802163e-01 2.72330371e-03 6.67851704e-01\n",
      " 6.27117686e-03 3.35706717e-01 8.01734477e-01 7.61924932e-01\n",
      " 3.10117987e-01 6.75044103e-01 4.94531093e-01 2.54348088e-01\n",
      " 6.57595419e-01 6.17449171e-01 4.23788238e-03 2.26790332e-01\n",
      " 6.98789370e-01 6.35224043e-04 3.17459404e-01 7.35148167e-01]\n",
      "\n",
      "Final weights:\n",
      " [-0.66157858 -1.37532609  1.6232512   0.12486542  0.00468821] \n",
      "\n",
      "\n",
      "Accuracy for class 3: 0.9000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "for i in range(3):\n",
    "    yBinary = np.where(y == i, 1, 0)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X, yBinary, test_size=0.2, random_state=42)\n",
    "    model = LogisticRegression(xTrain)\n",
    "    model.train(xTrain, yTrain)\n",
    "\n",
    "    yPred = model.predictClass(xTest)\n",
    "    accuracy = accuracy_score(yTest, yPred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    print(f\"\\nAccuracy for class {i+1}: {accuracy:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14d5fb9",
   "metadata": {},
   "source": [
    "# Final Accuracy of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6ef95f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average test accuracy: {np.mean(accuracies):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3dff60",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Note:** I have tried my best to provide accurate results in this notebook. However, these results may not be entirely accurate, and contributions or corrections are encouraged. Thank you!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
